{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpar5LziY_-0"
   },
   "source": [
    "#Zadanie 4 (7 pkt)\n",
    "Celem zadania jest zaimplementowanie algorytmu drzewa decyzyjnego ID3 dla zadania klasyfikacji. Trening i test należy przeprowadzić dla zbioru Iris. Proszę przeprowadzić eksperymenty najpierw dla DOKŁADNIE takiego podziału zbioru testowego i treningowego jak umieszczony poniżej. W dalszej części należy przeprowadzić analizę działania drzewa dla różnych wartości parametrów. Proszę korzystać z przygotowanego szkieletu programu, oczywiście można go modyfikować według potrzeb. Wszelkie elementy szkieletu zostaną wyjaśnione na zajęciach.\n",
    "\n",
    "* Implementacja funkcji entropii - **0.5 pkt**\n",
    "* Implementacja funkcji entropii zbioru - **0.5 pkt**\n",
    "* Implementacja funkcji information gain - **0.5 pkt**\n",
    "* Zbudowanie poprawnie działającego drzewa klasyfikacyjnego i przetestowanie go na wspomnianym wcześniej zbiorze testowym. Jeśli w liściu występuje kilka różnych klas, decyzją jest klasa większościowa. Policzenie accuracy i wypisanie parami klasy rzeczywistej i predykcji. - **4 pkt**\n",
    "* Przeprowadzenie eksperymentów dla różnych głębokości drzew i podziałów zbioru treningowego i testowego (zmiana wartości argumentu test_size oraz usunięcie random_state). W tym przypadku dla każdego eksperymentu należy wykonać kilka uruchomień programu i wypisać dla każdego uruchomienia accuracy. - **1.5 pkt**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "XNc-O3npA-J9"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_iris\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBh2tfQ44u5k"
   },
   "outputs": [],
   "source": [
    "def entropy_func(class_count, num_samples):\n",
    "    if class_count == 0:\n",
    "        return 0\n",
    "    p = class_count / num_samples\n",
    "    return -(p * np.log(p))\n",
    "\n",
    "\n",
    "class Group:\n",
    "    def __init__(self, group_classes):\n",
    "        self.group_classes = group_classes\n",
    "        self.entropy = self.group_entropy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.group_classes)\n",
    "\n",
    "    def group_entropy(self):\n",
    "        count_samples = Counter(self.group_classes)\n",
    "        num_samples = len(self)\n",
    "\n",
    "        sum = 0\n",
    "        for _, amt in count_samples.items():\n",
    "            sum += entropy_func(amt, num_samples)\n",
    "\n",
    "        return sum\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, split_feature, split_val, depth=None, child_node_a=None, child_node_b=None, val=None):\n",
    "        self.split_feature = split_feature # Index of the attribute in the data array\n",
    "        self.split_val = split_val\n",
    "        self.depth = depth\n",
    "        self.child_node_a = child_node_a # Left branch\n",
    "        self.child_node_b = child_node_b # Right branch\n",
    "        self.val = val\n",
    "\n",
    "    def predict(self, data):\n",
    "        if self.val is not None:\n",
    "            return self.val\n",
    "        return self.child_node_a.predict(data)\\\n",
    "    if data[self.split_feature] < self.split_val\\\n",
    "    else self.child_node_b.predict(data)\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier():\n",
    "    def __init__(self, max_depth):\n",
    "        self.depth = 0\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_split_entropy(group_a: Group, group_b: Group):\n",
    "        size = len(group_a) + len(group_b)\n",
    "        return (len(group_a) / size) * group_a.entropy + (len(group_b) / size) * group_b.entropy\n",
    "\n",
    "    def get_information_gain(self, parent_group: Group, child_group_a: Group, child_group_b: Group):\n",
    "        return parent_group.entropy - DecisionTreeClassifier.get_split_entropy(child_group_a, child_group_b)\n",
    "\n",
    "    def get_best_feature_split(self, feature_values, classes):\n",
    "        best_val = 0\n",
    "        max_inf_gain = float(\"-inf\")\n",
    "        for val in set(feature_values):\n",
    "            group_a = Group([classes[i] for i in range(len(classes)) if val > feature_values[i]])\n",
    "            group_b = Group([classes[i] for i in range(len(classes)) if val <= feature_values[i]])\n",
    "            parent_group = Group(classes)\n",
    "\n",
    "            inf_gain = self.get_information_gain(parent_group, group_a, group_b)\n",
    "            if inf_gain > max_inf_gain:\n",
    "                best_val = val\n",
    "                max_inf_gain = inf_gain\n",
    "\n",
    "        return best_val, max_inf_gain\n",
    "\n",
    "    def get_best_split(self, data, classes):\n",
    "        max_inf_gain = float(\"-inf\")\n",
    "        split_feature = 0\n",
    "        split_val = 0\n",
    "        for i in range(len(data[0])):\n",
    "            column = data[:, i]\n",
    "            val, inf_gain = self.get_best_feature_split(column, classes)\n",
    "            if inf_gain > max_inf_gain:\n",
    "                split_feature = i\n",
    "                split_val = val\n",
    "                max_inf_gain = inf_gain\n",
    "\n",
    "        return split_feature, split_val\n",
    "\n",
    "    def build_tree(self, data, classes, depth=0):\n",
    "        split_feature, split_val = self.get_best_split(np.array(data), classes)\n",
    "\n",
    "        child_a_indexes = data[:, split_feature] < split_val\n",
    "        child_b_indexes = data[:, split_feature] >= split_val\n",
    "\n",
    "        child_a_data = data[child_a_indexes]\n",
    "        child_a_classes = classes[child_a_indexes]\n",
    "        child_b_data = data[child_b_indexes]\n",
    "        child_b_classes = classes[child_b_indexes]\n",
    "        if (len(child_a_data) == 0 or len(child_b_data) == 0) and depth == 0:\n",
    "            most_common_class = Counter(classes).most_common(1)[0][0]\n",
    "            self.tree = Node(split_feature=None, split_val=None, val=most_common_class)\n",
    "\n",
    "        # we have reached max depth or its no use going deeper cause one of the child without data will cause errors\n",
    "        if depth >= self.max_depth or len(child_a_data) == 0 or len(child_b_data) == 0:\n",
    "            most_common_class = Counter(classes).most_common(1)[0][0]\n",
    "            return Node(split_feature=None, split_val=None, val=most_common_class)\n",
    "\n",
    "        child_node_a = self.build_tree(child_a_data, child_a_classes, depth+1)\n",
    "        child_node_b = self.build_tree(child_b_data, child_b_classes, depth+1)\n",
    "\n",
    "        if depth == 0:\n",
    "            # we have to assign root node\n",
    "            self.tree = Node(split_feature, split_val, 0, child_node_a, child_node_b)\n",
    "            return None\n",
    "        else:\n",
    "            return Node(split_feature, split_val, 0, child_node_a, child_node_b)\n",
    "\n",
    "\n",
    "    def predict(self, data):\n",
    "        return self.tree.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def run_predictions(self, x_train, y_train, x_test, y_test, max_depth=3, verbose=0, round_acc = None):\n",
    "        dc = DecisionTreeClassifier(max_depth)\n",
    "        dc.build_tree(x_train, y_train)\n",
    "        correct, count = 0, 0\n",
    "        for sample, gt in zip(x_test, y_test):\n",
    "\n",
    "            prediction = dc.predict(sample)\n",
    "            if verbose > 0:\n",
    "                print(f'Predicted: {prediction};\\tCorrect prediciton: {gt}')\n",
    "            count += 1\n",
    "            correct += 1 if prediction == gt else 0\n",
    "\n",
    "        accuracy = correct/count\n",
    "        if verbose > 0:\n",
    "            print(f'Accuracy: {accuracy}')\n",
    "        return round(accuracy, round_acc) if round_acc else accuracy\n",
    "\n",
    "    def simulate_depths(self, depths, x_test, y_test):\n",
    "        y_axis = [self.run_predictions(x_train, y_train, x_test, y_test, depth, 0, 2) for depth in depths]\n",
    "        self._plot_bar(depths, y_axis, \"depths\", \"accuracy\", \"Change of accurece depth\")\n",
    "\n",
    "    def simulate_training_set(self, train_sizes, depth):\n",
    "        x_train_set, y_train_set = self._get_set([0, 2], train_sizes, [None])\n",
    "\n",
    "        y_axis = [self.run_predictions(x_train, y_train, x_test, y_test, depth, 0, 2) for x_train, y_train in zip(x_train_set, y_train_set)]\n",
    "        self._plot_bar(train_sizes, y_axis, \"train_size\", \"accuracy\", f\"Change of accuracy due to training size, depth = {depth}\")\n",
    "\n",
    "    def simulate_test_set(self , test_sizes, depth, train_size = None):\n",
    "        y_axis = []\n",
    "        y_average = []\n",
    "        for i in range(100):\n",
    "            x_test_set, y_test_set = self._get_set([1, 3], [train_size], test_sizes)\n",
    "            y_axis.append([self.run_predictions(x_train, y_train, x_test, y_test, depth, 0, 2) for x_test, y_test in zip(x_test_set, y_test_set)])\n",
    "\n",
    "        for i in range(len(test_sizes)):\n",
    "            y_average[i] = np.average(y_axis, i)\n",
    "            \n",
    "        print(y_average)\n",
    "        self._plot_bar(test_sizes, y_average, \"test_size\", \"accuracy\", f\"Change of accuracy due to training size, depth = {depth}, train_size = {train_size}\")\n",
    "\n",
    "    def simulate_train_test_set(self, train_sizes, test_sizes, depth):\n",
    "        for element in train_sizes:\n",
    "            self.simulate_test_set(test_sizes, depth, train_size=element)\n",
    "\n",
    "    \"\"\"\n",
    "    ids - list of attributes you want\n",
    "    0 - x_train\n",
    "    1 - x_test\n",
    "    2 - y_train\n",
    "    3 - y_test\n",
    "    \"\"\"\n",
    "    def _get_set(self, ids, train_sizes=None, test_sizes=None):\n",
    "\n",
    "        result_set = {}\n",
    "        for id in ids:\n",
    "            result_set[id] = []\n",
    "\n",
    "        for train_size in train_sizes:\n",
    "            for test_size in test_sizes:\n",
    "                data = train_test_split(x, y, train_size=train_size, test_size=test_size)\n",
    "                for id in ids:\n",
    "                    result_set[id].append(data[id])\n",
    "\n",
    "        return tuple(result_set.values())\n",
    "\n",
    "\n",
    "\n",
    "    def _plot_bar(self, x_axis, y_axis, x_label, y_label, title):\n",
    "        bar_width = 0.55\n",
    "        x = np.arange(len(x_axis))\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(x, y_axis, width=bar_width, color='blue')\n",
    "\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text( bar.get_x() + bar.get_width()/2, height, f'{height}',\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        plt.title(title)\n",
    "        # plt.tick_params(axis='x', labelrotation=90)\n",
    "        plt.xticks(x, x_axis)  # Set x-axis tick labels to category names\n",
    "        plt.tight_layout()         # Adjust layout for better fit\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U033RY1_YS8x"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Solution' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[43mSolution\u001b[49m()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# s.simulate_depths(range(1, 9), x_test, y_test)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# s.simulate_training_set([1, 5, 10, 25, 50, 100], 3)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m s\u001b[38;5;241m.\u001b[39msimulate_test_set([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m], \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Solution' is not defined"
     ]
    }
   ],
   "source": [
    "s = Solution()\n",
    "# s.simulate_depths(range(1, 9), x_test, y_test)\n",
    "# s.simulate_training_set([1, 5, 10, 25, 50, 100], 3)\n",
    "s.simulate_test_set([1, 5, 10, 25, 50, 100], 3)\n",
    "# s.simulate_train_test_set([1, 5, 10, 25, 50], [1, 5, 10, 25, 50], 3)\n",
    "# dc.build_tree(x_train, y_train)\n",
    "# count = 0\n",
    "# correct = 0\n",
    "# for sample, gt in zip(x_test, y_test):\n",
    "#     count += 1\n",
    "#     prediction = dc.predict(sample)\n",
    "#     correct += 1 if prediction == gt else 0\n",
    "# print(correct, count)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernel_info": {
   "name": "python"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
