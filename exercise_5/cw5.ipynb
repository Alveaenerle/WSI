{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5BPd1IecgTcc"
      },
      "source": [
        "# Zadanie 5\n",
        "\n",
        "\n",
        "Celem ćwiczenia jest implementacja perceptronu wielowarstwowego oraz wybranego algorytmu optymalizacji gradientowej z algorytmem propagacji wstecznej.\n",
        "\n",
        "Następnie należy wytrenować perceptron wielowarstwowy do klasyfikacji zbioru danych [MNIST](http://yann.lecun.com/exdb/mnist/). Zbiór MNIST dostępny jest w pakiecie `scikit-learn`.\n",
        "\n",
        "Punktacja:\n",
        "1. Implementacja propagacji do przodu (`forward`) [1 pkt]\n",
        "2. Implementacja wstecznej propagacji (`backward`) [2 pkt]\n",
        "3. Przeprowadzenie eksperymentów na zbiorze MNIST, w tym:\n",
        "    1. Porównanie co najmniej dwóch architektur sieci [1 pkt]\n",
        "    2. Przetestowanie każdej architektury na conajmniej 3 ziarnach [1 pkt]\n",
        "    3. Wnioski [2.5 pkt]\n",
        "4. Jakość kodu [0.5 pkt]\n",
        "\n",
        "Polecane źródła - teoria + intuicja:\n",
        "1. [Karpathy, CS231n Winter 2016: Lecture 4: Backpropagation, Neural Networks 1](https://www.youtube.com/watch?v=i94OvYb6noo&ab_channel=AndrejKarpathy)\n",
        "2. [3 Blude one Brown, Backpropagation calculus | Chapter 4, Deep learning\n",
        "](https://www.youtube.com/watch?v=tIeHLnjs5U8&t=4s&ab_channel=3Blue1Brown)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNmMDnkRgTcj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\sledz\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n",
            "c:\\Users\\sledz\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from abc import abstractmethod, ABC\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load MNIST\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "x, y = mnist.data / 255.0, mnist.target.astype(int)\n",
        "y = y.to_numpy()\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Tl6Mj5wogTcm"
      },
      "outputs": [],
      "source": [
        "class Layer(ABC):\n",
        "    \"\"\"Basic building block of the Neural Network\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self._learning_rate = 0.01\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x:np.ndarray)->np.ndarray:\n",
        "        \"\"\"Forward propagation of x through layer\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, output_error_derivative) ->np.ndarray:\n",
        "        \"\"\"Backward propagation of output_error_derivative through layer\"\"\"\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    def learning_rate(self):\n",
        "        return self._learning_rate\n",
        "\n",
        "    @learning_rate.setter\n",
        "    def learning_rate(self, learning_rate):\n",
        "        assert learning_rate < 1, f\"Given learning_rate={learning_rate} is larger than 1\"\n",
        "        assert learning_rate > 0, f\"Given learning_rate={learning_rate} is smaller than 0\"\n",
        "        self._learning_rate = learning_rate\n",
        "\n",
        "class FullyConnected(Layer):\n",
        "    def __init__(self, input_size:int, output_size:int) -> None:\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
        "        self.biases = np.random.randn(output_size) * 0.01\n",
        "\n",
        "    def forward(self, x:np.ndarray)->np.ndarray:\n",
        "        self.input = x\n",
        "        self.output = np.dot(x, self.weights) + self.biases\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_error_derivative)->np.ndarray:\n",
        "        # Gradients for weights and biases\n",
        "        weights_gradient = np.dot(self.input.T, output_error_derivative)\n",
        "        weights_gradient = np.clip(weights_gradient, -1.0, 1.0)\n",
        "        biases_gradient = np.sum(output_error_derivative, axis=0, keepdims=False)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights -= self.learning_rate * weights_gradient\n",
        "        self.biases -= self.learning_rate * biases_gradient\n",
        "\n",
        "        # Propagate the error backward\n",
        "        return np.dot(output_error_derivative, self.weights.T)\n",
        "\n",
        "class Tanh(Layer):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x:np.ndarray)->np.ndarray:\n",
        "        self.input = x\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def backward(self, output_error_derivative)->np.ndarray:\n",
        "        tanh_derivative = 1 - np.tanh(self.input) ** 2\n",
        "        return output_error_derivative * tanh_derivative\n",
        "\n",
        "class Loss:\n",
        "    def __init__(self, loss_function:callable, loss_function_derivative:callable)->None:\n",
        "        self.loss_function = loss_function\n",
        "        self.loss_function_derivative = loss_function_derivative\n",
        "\n",
        "\n",
        "\n",
        "    def loss(self, y_pred:np.ndarray, y_true:np.ndarray)->np.ndarray:\n",
        "        \"\"\"Loss function for a particular x\"\"\"\n",
        "        return self.loss_function(y_pred, y_true)\n",
        "\n",
        "    def loss_derivative(self, y_pred:np.ndarray, y_true:np.ndarray)->np.ndarray:\n",
        "        \"\"\"Loss function derivative for a particular x and y\"\"\"\n",
        "        return self.loss_function_derivative(y_pred, y_true)\n",
        "\n",
        "class Mse(Loss):\n",
        "    def __init__(self):\n",
        "        super().__init__(self.mse, self.mse_derivative)\n",
        "\n",
        "    @staticmethod\n",
        "    def mse(y_pred, y_true):\n",
        "        return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "    @staticmethod\n",
        "    def mse_derivative(y_pred, y_true):\n",
        "        return 2 * (y_pred - y_true) / y_true.size\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, layers:List[Layer], learning_rate:float)->None:\n",
        "        self.layers = layers\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def compile(self, loss:Loss)->None:\n",
        "        \"\"\"Define the loss function and loss function derivative\"\"\"\n",
        "        self.loss = loss\n",
        "\n",
        "    def __call__(self, x:np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Forward propagation of x through all layers\"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def fit(self,\n",
        "            x_train:np.ndarray,\n",
        "            y_train:np.ndarray,\n",
        "            epochs:int,\n",
        "            learning_rate:float,\n",
        "            verbose:int=0)->None:\n",
        "        \"\"\"Fit the network to the training data\"\"\"\n",
        "\n",
        "        if learning_rate:\n",
        "            self.learning_rate = learning_rate\n",
        "            for layer in self.layers:\n",
        "                layer.learning_rate = self.learning_rate\n",
        "\n",
        "\n",
        "        accuracy_during_epoches = []\n",
        "        lost_during_epoches = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            correct_predictions = 0\n",
        "\n",
        "            # Forward pass\n",
        "            predictions = self(x_train)\n",
        "\n",
        "            # Compute loss\n",
        "            loss_value = self.loss.loss(predictions, y_train)\n",
        "\n",
        "            correct_predictions += (predictions.argmax(axis=1) == y_train.argmax(axis=1)).sum()\n",
        "\n",
        "            # Backward pass\n",
        "            loss_gradient = self.loss.loss_derivative(predictions, y_train)\n",
        "            for layer in reversed(self.layers):\n",
        "                loss_gradient = layer.backward(loss_gradient)\n",
        "            avg_accuracy = correct_predictions / len(x_train)\n",
        "            accuracy_during_epoches.append(avg_accuracy)\n",
        "            lost_during_epoches.append(total_loss)\n",
        "\n",
        "            # Verbose output\n",
        "            if verbose and epoch % verbose == 0:\n",
        "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss_value:.4f}, Accuracy: {avg_accuracy:.4f}\")\n",
        "\n",
        "        return accuracy_during_epoches, lost_during_epoches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "sZTneKpngTco"
      },
      "source": [
        "# Eksperymenty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "layer_accuracy = []\n",
        "layer_loss = []\n",
        "layers1 = [\n",
        "    FullyConnected(784, 128),\n",
        "    Tanh(),\n",
        "    FullyConnected(128, 32),\n",
        "    Tanh(),\n",
        "    FullyConnected(32, 10),\n",
        "    Tanh()\n",
        "]\n",
        "\n",
        "layers2 = [\n",
        "    FullyConnected(784, 64),\n",
        "    Tanh(),\n",
        "    FullyConnected(64, 10),\n",
        "    Tanh()\n",
        "]\n",
        "layers = [layers1,layers2]\n",
        "SEEDS = [0, 42, 100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40, Loss: 0.0999, Accuracy: 0.0987\n",
            "Epoch 2/40, Loss: 0.0995, Accuracy: 0.0987\n",
            "Epoch 3/40, Loss: 0.0991, Accuracy: 0.0987\n",
            "Epoch 4/40, Loss: 0.0987, Accuracy: 0.0987\n",
            "Epoch 5/40, Loss: 0.0984, Accuracy: 0.0987\n",
            "Epoch 6/40, Loss: 0.0980, Accuracy: 0.0987\n",
            "Epoch 7/40, Loss: 0.0977, Accuracy: 0.0987\n",
            "Epoch 8/40, Loss: 0.0974, Accuracy: 0.0987\n",
            "Epoch 9/40, Loss: 0.0971, Accuracy: 0.0987\n",
            "Epoch 10/40, Loss: 0.0968, Accuracy: 0.0987\n",
            "Epoch 11/40, Loss: 0.0965, Accuracy: 0.0987\n",
            "Epoch 12/40, Loss: 0.0963, Accuracy: 0.0987\n",
            "Epoch 13/40, Loss: 0.0960, Accuracy: 0.0987\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[55], line 20\u001b[0m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m Loss(\n\u001b[0;32m     15\u001b[0m     loss_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m y_pred, y_true: np\u001b[38;5;241m.\u001b[39mmean((y_pred \u001b[38;5;241m-\u001b[39m y_true) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     16\u001b[0m     loss_function_derivative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m y_pred, y_true: \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (y_pred \u001b[38;5;241m-\u001b[39m y_true) \u001b[38;5;241m/\u001b[39m y_true\u001b[38;5;241m.\u001b[39msize\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m network\u001b[38;5;241m.\u001b[39mcompile(loss)\n\u001b[1;32m---> 20\u001b[0m network\u001b[38;5;241m.\u001b[39mfit(x_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "Cell \u001b[1;32mIn[54], line 139\u001b[0m, in \u001b[0;36mNetwork.fit\u001b[1;34m(self, x_train, y_train, epochs, learning_rate, verbose)\u001b[0m\n\u001b[0;32m    137\u001b[0m loss_gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mloss_derivative(predictions, y_train)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m--> 139\u001b[0m     loss_gradient \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mbackward(loss_gradient)\n\u001b[0;32m    140\u001b[0m avg_accuracy \u001b[38;5;241m=\u001b[39m correct_predictions \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(x_train)\n\u001b[0;32m    141\u001b[0m accuracy_during_epoches\u001b[38;5;241m.\u001b[39mappend(avg_accuracy)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# Train the network\n",
        "network = Network(\n",
        "    layers=[FullyConnected(784, 128),\n",
        "    Tanh(),\n",
        "    FullyConnected(128, 32),\n",
        "    Tanh(),\n",
        "    FullyConnected(32, 10),\n",
        "    Tanh()],\n",
        "    learning_rate=0.01\n",
        ")\n",
        "SEEDS = [10, 50, 100]\n",
        "# Split into training and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.2, random_state=42)\n",
        "# Compile the network with the loss function\n",
        "loss = Loss(\n",
        "    loss_function=lambda y_pred, y_true: np.mean((y_pred - y_true) ** 2),\n",
        "    loss_function_derivative=lambda y_pred, y_true: 2 * (y_pred - y_true) / y_true.size\n",
        ")\n",
        "\n",
        "network.compile(loss)\n",
        "network.fit(x_train, y_train, epochs=40, learning_rate=0.1, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "UQ_vyk1bgTcp"
      },
      "source": [
        "# Wnioski"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
